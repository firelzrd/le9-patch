From 07b5d87bfaa6f1c82510a69399fc117739aeec87 Mon Sep 17 00:00:00 2001
From: Alexey Avramov <hakavlad@inbox.lv>
Date: Sun, 15 Aug 2021 18:07:38 +0900
Subject: [PATCH] mm/vmscan: add sysctl knobs for protecting the working set

The kernel does not provide a way to protect the working set under memory
pressure. A certain amount of anonymous and clean file pages is required by
the userspace for normal operation. First of all, the userspace needs a
cache of shared libraries and executable binaries. If the amount of the
clean file pages falls below a certain level, then thrashing and even
livelock can take place.

The patch provides sysctl knobs for protecting the working set (anonymous
and clean file pages) under memory pressure.

The vm.anon_min_kbytes sysctl knob provides *hard* protection of anonymous
pages. The anonymous pages on the on the current node won't be reclaimed
under any conditions when their amount is below vm.anon_min_kbytes. This
knob may be used to prevent excessive swap thrashing when anonymous
memory is low (for example, when memory is going to be overfilled by
compressed data of zram module). The default value is defined by
CONFIG_ANON_MIN_KBYTES (suggested 0 in Kconfig).

The vm.clean_low_kbytes sysctl knob provides *best-effort* protection of
clean file pages. The clean file pages on the current node won't be
reclaimed under memory pressure when their amount is below
vm.clean_low_kbytes *unless* we threaten to OOM or have no free swap space
or vm.swappiness=0. Protection of clean file pages using this knob may be
used when swapping is still possible to
  - prevent disk I/O thrashing under memory pressure;
  - improve performance in disk cache-bound tasks under memory pressure.
The default value is defined by CONFIG_CLEAN_LOW_KBYTES (suggested 0 in
Kconfig).

The vm.clean_min_kbytes sysctl knob provides *hard* protection of clean
file pages. The clean file pages on the current node won't be reclaimed
under memory pressure when their amount is below vm.clean_min_kbytes.
Hard protection of clean file pages using this knob may be used to
  - prevent disk I/O thrashing under memory pressure even with no swap
    space;
  - improve performance in disk cache-bound tasks under memory pressure;
  - avoid high latency and prevent livelock in near-OOM conditions.
The default value is defined by CONFIG_CLEAN_MIN_KBYTES (suggested 0 in
Kconfig).

Signed-off-by: Alexey Avramov <hakavlad@inbox.lv>
---
 Documentation/sysctl/vm.txt | 59 ++++++++++++++++++++++++++++
 include/linux/mm.h          |  4 ++
 kernel/sysctl.c             | 21 ++++++++++
 mm/Kconfig                  | 61 +++++++++++++++++++++++++++++
 mm/vmscan.c                 | 94 +++++++++++++++++++++++++++++++++++++++++++++
 5 files changed, 239 insertions(+)

diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt
index 7d73882e2..b2cc151e9 100644
--- a/Documentation/sysctl/vm.txt
+++ b/Documentation/sysctl/vm.txt
@@ -19,7 +19,10 @@ files can be found in mm/swap.c.
 Currently, these files are in /proc/sys/vm:

 - admin_reserve_kbytes
+- anon_min_kbytes
 - block_dump
+- clean_low_kbytes
+- clean_min_kbytes
 - compact_memory
 - compact_unevictable_allowed
 - dirty_background_bytes
@@ -97,6 +100,23 @@ Changing this takes effect whenever an application requests memory.

 ==============================================================

+anon_min_kbytes
+
+This knob provides *hard* protection of anonymous pages. The anonymous pages
+on the on the current node won't be reclaimed under any conditions when their
+amount is below vm.anon_min_kbytes.
+
+This knob may be used to prevent excessive swap thrashing when anonymous
+memory is low (for example, when memory is going to be overfilled by
+compressed data of zram module).
+
+Setting this value too high (up to MemTotal and higher) can result in
+inability to swap and can lead to early OOM under memory pressure.
+
+The default value is defined by CONFIG_ANON_MIN_KBYTES.
+
+==============================================================
+
 block_dump

 block_dump enables block I/O debugging when set to a nonzero value. More
@@ -104,6 +124,45 @@ information on block I/O debugging is in Documentation/laptops/laptop-mode.txt.

 ==============================================================

+clean_low_kbytes
+
+This knob provides *best-effort* protection of clean file pages. The clean file
+pages on the current node won't be reclaimed under memory pressure when their
+amount is below vm.clean_low_kbytes *unless* we threaten to OOM or have no
+free swap space or vm.swappiness=0.
+
+Protection of clean file pages using this knob may be used when swapping is
+still possible to
+  - prevent disk I/O thrashing under memory pressure;
+  - improve performance in disk cache-bound tasks under memory pressure.
+
+Setting it to a high value may result in a early eviction of anonymous pages
+into the swap space by attempting to hold the protected amount of clean file
+pages in memory.
+
+The default value is defined by CONFIG_CLEAN_LOW_KBYTES.
+
+==============================================================
+
+clean_min_kbytes
+
+This knob provides *hard* protection of clean file pages. The clean file pages
+on the current node won't be reclaimed under memory pressure when their amount
+is below vm.clean_min_kbytes.
+
+Hard protection of clean file pages using this knob may be used to
+  - prevent disk I/O thrashing under memory pressure even with no swap space;
+  - improve performance in disk cache-bound tasks under memory pressure;
+  - avoid high latency and prevent livelock in near-OOM conditions.
+
+Setting it to a high value may result in a early out-of-memory condition due to
+the inability to reclaim the protected amount of clean file pages when other
+types of pages cannot be reclaimed.
+
+The default value is defined by CONFIG_CLEAN_MIN_KBYTES.
+
+==============================================================
+
 compact_memory

 Available only when CONFIG_COMPACTION is set. When 1 is written to the file,
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 0416a7204..5cbc6f711 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -126,6 +126,10 @@ extern int mmap_rnd_compat_bits __read_mostly;

 extern int sysctl_max_map_count;

+extern unsigned long sysctl_anon_min_kbytes;
+extern unsigned long sysctl_clean_low_kbytes;
+extern unsigned long sysctl_clean_min_kbytes;
+
 extern unsigned long sysctl_user_reserve_kbytes;
 extern unsigned long sysctl_admin_reserve_kbytes;

diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index cc02050fd..9590abe71 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1629,6 +1629,27 @@ static struct ctl_table vm_table[] = {
 	},
 #endif
 	{
+		.procname	= "anon_min_kbytes",
+		.data		= &sysctl_anon_min_kbytes,
+		.maxlen		= sizeof(unsigned long),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+	},
+	{
+		.procname	= "clean_low_kbytes",
+		.data		= &sysctl_clean_low_kbytes,
+		.maxlen		= sizeof(unsigned long),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+	},
+	{
+		.procname	= "clean_min_kbytes",
+		.data		= &sysctl_clean_min_kbytes,
+		.maxlen		= sizeof(unsigned long),
+		.mode		= 0644,
+		.proc_handler	= proc_doulongvec_minmax,
+	},
+	{
 		.procname	= "user_reserve_kbytes",
 		.data		= &sysctl_user_reserve_kbytes,
 		.maxlen		= sizeof(sysctl_user_reserve_kbytes),
diff --git a/mm/Kconfig b/mm/Kconfig
index de64ea658..17e5eca3a 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -127,6 +127,67 @@ config SPARSEMEM_VMEMMAP
 	 pfn_to_page and page_to_pfn operations.  This is the most
 	 efficient option when sufficient kernel resources are available.

+config ANON_MIN_KBYTES
+	int "Default value for vm.anon_min_kbytes"
+	depends on SYSCTL
+	range 0 4294967295
+	default 0
+	help
+	  This option sets the default value for vm.anon_min_kbytes sysctl knob.
+
+	  The vm.anon_min_kbytes sysctl knob provides *hard* protection of
+	  anonymous pages. The anonymous pages on the on the current node won't
+	  be reclaimed under any conditions when their amount is below
+	  vm.anon_min_kbytes. This knob may be used to prevent excessive swap
+	  thrashing when anonymous memory is low (for example, when memory is
+	  going to be overfilled by compressed data of zram module).
+
+	  Setting this value too high (up to MemTotal and higher) can result in
+	  inability to swap and can lead to early OOM under memory pressure.
+
+config CLEAN_LOW_KBYTES
+	int "Default value for vm.clean_low_kbytes"
+	depends on SYSCTL
+	range 0 4294967295
+	default 0
+	help
+	  This option sets the default value for vm.clean_low_kbytes sysctl knob.
+
+	  The vm.clean_low_kbytes sysctl knob provides *best-effort*
+	  protection of clean file pages. The clean file pages on the current
+	  node won't be reclaimed under memory pressure when their amount is
+	  below vm.clean_low_kbytes *unless* we threaten to OOM or have
+	  no free swap space or vm.swappiness=0. Protection of clean file pages
+	  using this knob may be used when swapping is still possible to
+	    - prevent disk I/O thrashing under memory pressure;
+	    - improve performance in disk cache-bound tasks under memory pressure.
+
+	  Setting it to a high value may result in a early eviction of anonymous
+	  pages into the swap space by attempting to hold the protected amount
+	  of clean file pages in memory.
+
+config CLEAN_MIN_KBYTES
+	int "Default value for vm.clean_min_kbytes"
+	depends on SYSCTL
+	range 0 4294967295
+	default 0
+	help
+	  This option sets the default value for vm.clean_min_kbytes sysctl knob.
+
+	  The vm.clean_min_kbytes sysctl knob provides *hard* protection
+	  of clean file pages. The clean file pages on the current node won't be
+	  reclaimed under memory pressure when their amount is below
+	  vm.clean_min_kbytes. Hard protection of clean file pages using this
+	  knob may be used to
+	    - prevent disk I/O thrashing under memory pressure even with
+	      no swap space;
+	    - improve performance in disk cache-bound tasks under memory pressure;
+	    - avoid high latency and prevent livelock in near-OOM conditions.
+
+	  Setting it to a high value may result in a early out-of-memory condition
+	  due to the inability to reclaim the protected amount of clean file pages
+	  when other types of pages cannot be reclaimed.
+
 config HAVE_MEMBLOCK
 	bool

diff --git a/mm/vmscan.c b/mm/vmscan.c
index c5ef7240c..2f473c34b 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -99,6 +99,25 @@ struct scan_control {
 	/* One of the zones is ready for compaction */
 	unsigned int compaction_ready:1;

+	/*
+	 * The anonymous pages on the current node won't be reclaimed when
+	 * their amount is below vm.anon_min_kbytes.
+	 */
+	unsigned int anon_below_min:1;
+
+	/*
+	 * The clean file pages on the current node won't be reclaimed when
+	 * their amount is below vm.clean_low_kbytes *unless* we threaten
+	 * to OOM or have no free swap space or vm.swappiness=0.
+	 */
+	unsigned int clean_below_low:1;
+
+	/*
+	 * The clean file pages on the current node won't be reclaimed when
+	 * their amount is below vm.clean_min_kbytes.
+	 */
+	unsigned int clean_below_min:1;
+
 	/* Allocation order */
 	s8 order;

@@ -156,6 +175,10 @@ struct scan_control {
 #define prefetchw_prev_lru_page(_page, _base, _field) do { } while (0)
 #endif

+unsigned long sysctl_anon_min_kbytes __read_mostly = CONFIG_ANON_MIN_KBYTES;
+unsigned long sysctl_clean_low_kbytes __read_mostly = CONFIG_CLEAN_LOW_KBYTES;
+unsigned long sysctl_clean_min_kbytes __read_mostly = CONFIG_CLEAN_MIN_KBYTES;
+
 /*
  * From 0 .. 100.  Higher means more swappy.
  */
@@ -2264,6 +2287,54 @@ static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
 	return shrink_inactive_list(nr_to_scan, lruvec, sc, lru);
 }

+static void prepare_workingset_protection(pg_data_t *pgdat, struct scan_control *sc)
+{
+	/*
+	 * Check the number of anonymous pages to protect them from
+	 * reclaiming if their amount is below the specified.
+	 */
+	if (sysctl_anon_min_kbytes) {
+		unsigned long reclaimable_anon;
+
+		reclaimable_anon =
+			node_page_state(pgdat, NR_ACTIVE_ANON) +
+			node_page_state(pgdat, NR_INACTIVE_ANON) +
+			node_page_state(pgdat, NR_ISOLATED_ANON);
+		reclaimable_anon <<= (PAGE_SHIFT - 10);
+
+		sc->anon_below_min = reclaimable_anon < sysctl_anon_min_kbytes;
+	} else
+		sc->anon_below_min = 0;
+
+	/*
+	 * Check the number of clean file pages to protect them from
+	 * reclaiming if their amount is below the specified.
+	 */
+	if (sysctl_clean_low_kbytes || sysctl_clean_min_kbytes) {
+		unsigned long reclaimable_file, dirty, clean;
+
+		reclaimable_file =
+			node_page_state(pgdat, NR_ACTIVE_FILE) +
+			node_page_state(pgdat, NR_INACTIVE_FILE) +
+			node_page_state(pgdat, NR_ISOLATED_FILE);
+		dirty = node_page_state(pgdat, NR_FILE_DIRTY);
+		/*
+		 * node_page_state() sum can go out of sync since
+		 * all the values are not read at once.
+		 */
+		if (likely(reclaimable_file > dirty))
+			clean = (reclaimable_file - dirty) << (PAGE_SHIFT - 10);
+		else
+			clean = 0;
+
+		sc->clean_below_low = clean < sysctl_clean_low_kbytes;
+		sc->clean_below_min = clean < sysctl_clean_min_kbytes;
+	} else {
+		sc->clean_below_low = 0;
+		sc->clean_below_min = 0;
+	}
+}
+
 enum scan_balance {
 	SCAN_EQUAL,
 	SCAN_FRACT,
@@ -2295,6 +2366,8 @@ static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 	unsigned long ap, fp;
 	enum lru_list lru;

+	prepare_workingset_protection(pgdat, sc);
+
 	/* If we have no swap space, do not bother scanning anon pages. */
 	if (!sc->may_swap || mem_cgroup_get_nr_swap_pages(memcg) <= 0) {
 		scan_balance = SCAN_FILE;
@@ -2366,6 +2439,16 @@ static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 	}

 	/*
+	 * Force-scan anon if clean file pages is under vm.clean_min_kbytes
+	 * or vm.clean_low_kbytes (unless the swappiness setting
+	 * disagrees with swapping).
+	 */
+	if ((sc->clean_below_low || sc->clean_below_min) && swappiness) {
+		scan_balance = SCAN_ANON;
+		goto out;
+	}
+
+	/*
 	 * If there is enough inactive page cache, i.e. if the size of the
 	 * inactive list is greater than that of the active list *and* the
 	 * inactive list actually has some pages to scan on this priority, we
@@ -2473,6 +2556,17 @@ static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 			BUG();
 		}

+		/*
+		 * Hard protection of anonymous and clean file pages.
+		 */
+		if (file) {
+			if (sc->clean_below_min)
+				scan = 0;
+		} else {
+			if (sc->anon_below_min)
+				scan = 0;
+		}
+
 		*lru_pages += size;
 		nr[lru] = scan;
 	}

base-commit: 84df9525b0c27f3ebc2ebb1864fa62a97fdedb7d
--
2.11.0

